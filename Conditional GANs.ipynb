{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.functional import one_hot, leaky_relu, interpolate\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "from tqdm.notebook import trange\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # conf\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TRAIN_CGAN = True\n",
    "TRAIN_RESNET = True\n",
    "TRAIN_AUGMENTED_RESNET = True\n",
    "\n",
    "RESNET = \"resnet\"\n",
    "CONV = \"conv\"\n",
    "architecture = os.getenv('ARCH', RESNET)  # resnet or conv\n",
    "tag = os.getenv(\"TAG\", None)\n",
    "tag = f\"_{tag}\" if tag else \"\"\n",
    "# num of workers for the dataloader\n",
    "num_workers = 4 if architecture == RESNET else 8\n",
    "generated_images_dir = f\"CGAN/generated_images/{architecture}{tag}\"\n",
    "Path(generated_images_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(\"CGAN/models\").mkdir(exist_ok=True)\n",
    "gan_model_path = f\"CGAN/models/generator_{architecture}{tag}.pth\"\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "LATENT_DIM = 100  # Dimensions of noise Z\n",
    "NUM_CLASSES = 20\n",
    "IMG_SIZE = 128\n",
    "CHANNELS = 3  # RGB\n",
    "g_lr = 0.0005 if architecture == CONV else 0.0003\n",
    "d_lr = 0.00051 if architecture == CONV else 0.00031\n",
    "b1, b2 = 0.5, 0.99\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 64 if architecture == RESNET else 128\n",
    "# Number of training iterations for discriminator for each iteration of generator\n",
    "D_TURNS = 5 if architecture == RESNET else 3\n",
    "lambda_gp = 10  # Hyper-parameter for gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard stuff\n",
    "writer = SummaryWriter(log_dir=f'CGAN/tensorboard/{architecture}{tag}')\n",
    "\n",
    "\n",
    "def log_losses_to_tensorboard(epoch, g_loss, d_loss):\n",
    "    writer.add_scalar('Loss/Generator', g_loss, epoch)\n",
    "    writer.add_scalar('Loss/Discriminator', d_loss, epoch)\n",
    "\n",
    "\n",
    "def log_gradients_to_tensorboard(model, epoch, model_name):\n",
    "    total_norm = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            norm = param.grad.norm(2).item()\n",
    "            total_norm += norm ** 2\n",
    "            writer.add_scalar(f'Gradients/{model_name}/{name}', norm, epoch)\n",
    "    total_norm = total_norm ** 0.5\n",
    "    writer.add_scalar(f'Gradients/{model_name}/total_norm', total_norm, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/classes.txt\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    # Load the class labels from the text file\n",
    "    with open(filename, 'r') as file:\n",
    "        classes = [line.strip() for line in file.readlines()]\n",
    "else:\n",
    "    with open('data/Animals_data/name of the animals.txt', 'r') as animals:\n",
    "        animal_names = [name.strip() for name in animals.readlines()]\n",
    "        classes = random.sample(animal_names, NUM_CLASSES)\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write('\\n'.join(classes) + '\\n')\n",
    "\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None):\n",
    "        self.root_dir = root_dir  # Root directory containing the image data\n",
    "        self.transform = transform  # Image transformations to be applied\n",
    "        self.classes = classes  # List of class names\n",
    "        self.image_paths = []  # List to store paths of all images\n",
    "        self.labels = []  # List to store corresponding labels\n",
    "\n",
    "        # Iterate through each class and collect image paths and labels\n",
    "        for class_id, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                self.labels.append(class_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)  # Return the total number of images in the dataset\n",
    "\n",
    "    def preload(self):\n",
    "        # Preload all images into memory (useful for small datasets)\n",
    "        self.preloaded_images = [Image.open(path).convert(\n",
    "            'RGB') for path in self.image_paths]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.image_paths[idx]  # Get the path of the image at index idx\n",
    "        image = Image.open(src).convert('RGB')  # Open the image and convert to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations if specified\n",
    "        label = self.labels[idx]  # Get the corresponding label\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def augment_with_gan(self, generator, samples_per_class):\n",
    "        generator.eval()  # Set generator to evaluation mode\n",
    "\n",
    "        for class_idx in range(len(self.classes)):\n",
    "            class_path = f\"CGAN/augmentation_data/{self.classes[class_idx]}\"\n",
    "            Path(class_path).mkdir(parents=True, exist_ok=True)  # Create directory for augmented images\n",
    "\n",
    "            # Generate noise and class labels for the generator\n",
    "            noise = torch.randn(samples_per_class, LATENT_DIM).to(device)\n",
    "            class_labels = torch.full(\n",
    "                (samples_per_class,), class_idx).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake_samples = generator(noise, class_labels)  # Generate fake samples\n",
    "                fake_samples = (fake_samples + 1) / 2.0  # Denormalize the generated images\n",
    "\n",
    "            # Save generated images and update dataset\n",
    "            for i, fake_sample in enumerate(fake_samples):\n",
    "                fake_sample = to_pil_image(fake_sample)\n",
    "                path = f\"{class_path}/{i}.jpg\"\n",
    "                fake_sample.save(path)\n",
    "                self.image_paths.append(path)\n",
    "                self.labels.append(class_idx)\n",
    "\n",
    "\n",
    "# Define image transformations\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),  # To preserve aspect ratio of landscape oriented images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # Normalize to [-1, 1] range\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = AnimalDataset(root_dir='data/Animals_data/animals/animals/',\n",
    "                        classes=classes, transform=img_transform)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                        num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models\n",
    "- There are two models we need:\n",
    "    1. A _generator_ that takes in random noise (z) and a class label as input and generates a sample image\n",
    "    2. A _discriminator_ that takes in an image and class label as input and outputs a score signifying \"realness\" of the image.\n",
    "- We perform experiments with two different architecture variants:\n",
    "    1. **Convolution Based (DCGAN)**: Less complex and memory efficient\n",
    "    2. **Resnet Based**: More complex but requires a lot more memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "- The label is converted to an embedding/one-hot vector and concatenated with z\n",
    "- The concatenated input is then passed to a linear layer which outputs a tensor of shape `(512, init_size, init_size)` where `init_size X init_size` is the initial size of 2D image\n",
    "- The image is then passed through multiple convolutional blocks via upsampling till we get an image with 3 channels (RGB)\n",
    "- The last layer is an tanH activation layer so that our RGB values are normalized to [-1, 1] which is the same normalization as the real images\n",
    "- Instead of regular batch norm, we use conditional batch norm that learns a different set of normalization parameters of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)  # Initialize weights of Conv2d and ConvTranspose2d layers with normal distribution (mean=0.0, std=0.02)\n",
    "    elif (isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d)) and m.weight is not None:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)  # Initialize weights of BatchNorm2d and InstanceNorm2d layers with normal distribution (mean=1.0, std=0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)  # Initialize biases of BatchNorm2d and InstanceNorm2d layers to 0\n",
    "\n",
    "\n",
    "# Taken from: https://github.com/pytorch/pytorch/issues/8985#issuecomment-405080775\n",
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.num_features = num_features  # Store number of features\n",
    "        self.bn = nn.BatchNorm2d(num_features, affine=False)  # Create BatchNorm2d layer without learnable affine parameters\n",
    "        self.embed = nn.Embedding(num_classes, num_features * 2)  # Create embedding layer for class-conditional parameters\n",
    "        self.embed.weight.data[:, :num_features].normal_(\n",
    "            1, 0.02)  # Initialize scale parameters of embedding with normal distribution (mean=1, std=0.02)\n",
    "        self.embed.weight.data[:, num_features:].zero_()  # Initialize bias parameters of embedding to 0\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        out = self.bn(x)  # Apply batch normalization\n",
    "        gamma, beta = self.embed(labels).chunk(2, 1)  # Get class-conditional scale and bias parameters\n",
    "        out = gamma.reshape(-1, self.num_features, 1, 1) * out + \\\n",
    "            beta.reshape(-1, self.num_features, 1, 1)  # Apply class-conditional affine transformation\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, channels):\n",
    "        super(ConditionalGenerator, self).__init__()  # Initialize parent class\n",
    "        num_blocks = 5 # no of upsamples\n",
    "        self.init_size = IMG_SIZE // (2**num_blocks)  # Calculate initial size of feature maps\n",
    "        self.num_classes = num_classes  # Store number of classes\n",
    "\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 512 * self.init_size * self.init_size))  # Initial linear layer to project input to desired shape\n",
    "\n",
    "        self.cbn1 = ConditionalBatchNorm2d(512, num_classes)  # First conditional batch norm layer\n",
    "\n",
    "        self.cbns = nn.ModuleList([\n",
    "            ConditionalBatchNorm2d(256, num_classes),\n",
    "            ConditionalBatchNorm2d(128, num_classes),\n",
    "            ConditionalBatchNorm2d(64, num_classes),\n",
    "            ConditionalBatchNorm2d(32, num_classes),\n",
    "        ])  # List of conditional batch norm layers for each block\n",
    "        self.relus = nn.ModuleList(\n",
    "            [nn.LeakyReLU(0.2, inplace=True) for _ in range(num_blocks-1)])  # List of LeakyReLU activation functions\n",
    "\n",
    "        self.upsamplers = nn.ModuleList(\n",
    "            [nn.Upsample(scale_factor=2) for _ in range(num_blocks-1)]\n",
    "        )  # List of upsampling layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(512, 256, 5, stride=1, padding=2),\n",
    "            nn.Conv2d(256, 128, 5, stride=1, padding=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "        ]\n",
    "        )  # List of convolutional layers\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )  # Output layer with final upsampling, convolution, and Tanh activation\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embeddings = one_hot(labels, self.num_classes)  # Convert labels to one-hot encoding\n",
    "        combined = torch.cat([z, label_embeddings], dim=1)  # Concatenate noise and label embeddings\n",
    "        out = self.l1(combined)  # Pass through initial linear layer\n",
    "        out = out.view(out.size(0), 512, self.init_size, self.init_size)  # Reshape output to 4D tensor\n",
    "        out = self.cbn1(out, labels)  # Apply first conditional batch norm\n",
    "        for upsampler, conv, cbn, relu in zip(self.upsamplers, self.convs, self.cbns, self.relus):\n",
    "            out = upsampler(out)  # Upsample\n",
    "            out = conv(out)  # Apply convolution\n",
    "            out = cbn(out, labels)  # Apply conditional batch norm\n",
    "            out = relu(out)  # Apply LeakyReLU activation\n",
    "\n",
    "        out = self.out_layer(out)  # Pass through final output layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResUpBlock class is a building block for the ResNet-style generator in a Conditional GAN (Generative Adversarial Network). It implements a residual upsampling block that combines convolutional layers with conditional batch normalization and skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mResidualUpsamplingBlock\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):  \u001b[38;5;66;03m# Define a class for the residual upsampling block\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_channels, output_channels):  \u001b[38;5;66;03m# Initialize with input and output channels\u001b[39;00m\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()  \u001b[38;5;66;03m# Call the parent class constructor\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ResUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.cond_bns = nn.ModuleList([\n",
    "            ConditionalBatchNorm2d(in_channels, NUM_CLASSES),\n",
    "            ConditionalBatchNorm2d(out_channels, NUM_CLASSES)\n",
    "        ])\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1),\n",
    "        ])\n",
    "\n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0)\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.convs.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight, math.sqrt(2))\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        for m in self.skip.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        out = self.cond_bns[0](x, labels)\n",
    "        out = leaky_relu(out, 0.2)\n",
    "        out = interpolate(out, scale_factor=2)\n",
    "        out = self.convs[0](out)\n",
    "        out = self.cond_bns[1](out, labels)\n",
    "        out = leaky_relu(out, 0.2)\n",
    "        out = self.convs[1](out)\n",
    "        return out + self.skip(x)\n",
    "\n",
    "\n",
    "class ResGenerator(nn.Module):\n",
    "    def __init__(self, z_dim, num_classes, channels):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.num_classes = num_classes\n",
    "        num_blocks = 4 # no of downsamples\n",
    "        self.init_size = IMG_SIZE // (2**num_blocks)\n",
    "\n",
    "        self.l1 = nn.Linear(z_dim + num_classes,\n",
    "                            self.init_size * self.init_size * 512)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResUpBlock(512, 256),\n",
    "            ResUpBlock(256, 128),\n",
    "            ResUpBlock(128, 64),\n",
    "            ResUpBlock(64, 32),\n",
    "        ]\n",
    "        )\n",
    "        self.cbn = ConditionalBatchNorm2d(32, num_classes)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        torch.nn.init.xavier_uniform_(self.l1.weight)\n",
    "        torch.nn.init.zeros_(self.l1.bias)\n",
    "        for m in self.output.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embeddings = one_hot(labels, self.num_classes)\n",
    "        z = torch.cat([z, label_embeddings], dim=1)\n",
    "        z = self.l1(z)\n",
    "        z = z.view(-1, 512, self.init_size, self.init_size)\n",
    "        for block in self.blocks:\n",
    "            z = block(z, labels)\n",
    "        z = self.cbn(z, labels)\n",
    "        return self.output(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "- The image is first downsampled through multiple convolutional blocks till we get a set of images features in a single channel\n",
    "- The features are then concatenated with the label embedding\n",
    "- The concatenation is then pass through a linear layer which outputs a scalar value\n",
    "- For the resnet variant, instead of concatenating the feature maps with the label embedding, we add a projection of the features on the label embedding to the final scalar output. According to [Miyato et al. 2018](https://arxiv.org/abs/1802.05637), this approach is a better way encode class information compared to concatenation since it allows a linear interaction between the class label and the image features. Projection works well with the resnet variant because the pooling layers aggregate spatial information into a single vector. Whereas, the convolution architecture retains spatial information and flattening the structure for projection would lead to a loss of spatial information.\n",
    "- The label embeddings are generate from a static embedding layer where the class embeddings are orthogonal to each other. This is done to maintain class separability in the image features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training C-GAN\n",
    "- We train using Wasserstein's distance as our loss function\n",
    "- The discriminator is trained `d_turns` times more than the generator to prevent the generator from overpowering\n",
    "- We also apply [gradient penalty](https://arxiv.org/abs/1704.00028) to the discriminator loss to prevent overfitting.\n",
    "- Our dataset is quite limited so we apply augmentations to both the real and fake images for better generalization. Since backpropagation needs to accomodate for the augmentation information when updating the generator, our augmentations need to be differentiable. This idea directly comes from [Differentiable Augmentation for Data-Efficient GAN Training](https://arxiv.org/abs/2006.10738)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable Augmentation for Data-Efficient GAN Training\n",
    "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
    "# https://arxiv.org/pdf/2006.10738\n",
    "# Code take from: https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_pytorch.py\n",
    "\n",
    "def DiffAugment(x, policy='color,translation,cutout', channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(','):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = torch.nn.functional.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples periodically to visually track progress\n",
    "def generate_and_save_images(generator, epoch, display=False):\n",
    "    # Generate random noise for the generator\n",
    "    with torch.no_grad():\n",
    "        # Generate 5 samples for each class\n",
    "        samples_per_class = 5\n",
    "        total_samples = NUM_CLASSES * samples_per_class\n",
    "        z = torch.randn(total_samples, LATENT_DIM, device=device)\n",
    "        labels = torch.tensor(list(range(NUM_CLASSES)) *\n",
    "                              samples_per_class, device=device)\n",
    "\n",
    "        # Generate images\n",
    "        generated_images = generator(z, labels)\n",
    "\n",
    "        # Move images to CPU and convert to numpy arrays\n",
    "        generated_images = generated_images.cpu().numpy()\n",
    "\n",
    "        # Rescale images from [-1, 1] to [0, 1]\n",
    "        generated_images = (generated_images + 1) / 2.0\n",
    "\n",
    "        # Plot images\n",
    "        fig, axes = plt.subplots(NUM_CLASSES, samples_per_class, figsize=(\n",
    "            samples_per_class*2, NUM_CLASSES*2))\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            img = np.transpose(generated_images[i], (1, 2, 0))\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            if i % samples_per_class == 0:\n",
    "                ax.set_title(\n",
    "                    f\"Class {dataset.classes[i // samples_per_class]}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.title(f'Epoch {epoch}')\n",
    "        plt.savefig(f'{generated_images_dir}/{epoch}.jpg')\n",
    "        if display:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from [1704.00028] Improved Training of Wasserstein GANs\n",
    "def gradient_penalty(discriminator, real_samples, fake_samples, labels):\n",
    "    batch_size = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha)\n",
    "                    * fake_samples).requires_grad_(True)\n",
    "    c_interpolates = discriminator(interpolates, labels)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=c_interpolates, inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(c_interpolates),\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty * lambda_gp\n",
    "\n",
    "\n",
    "def train_cgan(generator, discriminator):\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=g_lr, betas=(b1, b2))\n",
    "    optimizer_D = optim.Adam(\n",
    "        discriminator.parameters(), lr=d_lr, betas=(b1, b2))\n",
    "\n",
    "    scheduler_G = optim.lr_scheduler.StepLR(\n",
    "        optimizer_G, step_size=N_EPOCHS//4, gamma=0.5)\n",
    "    scheduler_D = optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, step_size=N_EPOCHS//4, gamma=0.5)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in trange(N_EPOCHS, desc=\"Training Progress\"):\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            batch_size = images.size(0)\n",
    "            labels = labels.to(device)\n",
    "            images = images.to(device)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            optimizer_D.zero_grad()\n",
    "            z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
    "            gen_imgs = generator(z, labels)\n",
    "            images_aug = DiffAugment(images)\n",
    "            gen_imgs_aug = DiffAugment(gen_imgs)\n",
    "            real_validity = discriminator(images_aug, labels)\n",
    "            fake_validity = discriminator(gen_imgs_aug.detach(), labels)\n",
    "\n",
    "            gp = gradient_penalty(discriminator,\n",
    "                                  images_aug.data, gen_imgs_aug.data, labels)\n",
    "\n",
    "            d_loss = -torch.mean(real_validity) + \\\n",
    "                torch.mean(fake_validity) + gp\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            if i % D_TURNS == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "                z = torch.randn(batch_size, LATENT_DIM).to(device)\n",
    "                gen_imgs = generator(z, labels)\n",
    "                g_loss = - \\\n",
    "                    torch.mean(discriminator(DiffAugment(gen_imgs), labels))\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "        scheduler_D.step()\n",
    "        scheduler_G.step()\n",
    "\n",
    "        log_losses_to_tensorboard(epoch, g_loss.item(), d_loss.item())\n",
    "        log_gradients_to_tensorboard(generator, epoch, 'Generator')\n",
    "        log_gradients_to_tensorboard(\n",
    "            discriminator, epoch, 'Discriminator')\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch [{epoch}/{N_EPOCHS}]  D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "            generate_and_save_images(generator, epoch)\n",
    "\n",
    "\n",
    "if TRAIN_CGAN:\n",
    "    # Instantiate generator and discriminator\n",
    "    if architecture == RESNET:\n",
    "        # One GPU is not enough for the resnet GAN so use DataParallel\n",
    "        generator = nn.DataParallel(ResGenerator(\n",
    "            LATENT_DIM, NUM_CLASSES, CHANNELS)).to(device)\n",
    "        discriminator = nn.DataParallel(\n",
    "            ResDiscriminator(NUM_CLASSES, CHANNELS)).to(device)\n",
    "    else:\n",
    "        generator = ConditionalGenerator(\n",
    "            LATENT_DIM, NUM_CLASSES, CHANNELS).to(device)\n",
    "        discriminator = ConditionalDiscriminator(\n",
    "            NUM_CLASSES, CHANNELS).to(device)\n",
    "        generator.apply(init_weights)\n",
    "        discriminator.apply(init_weights)\n",
    "\n",
    "    train_cgan(generator, discriminator)\n",
    "    torch.save(generator.state_dict(), gan_model_path)\n",
    "    print(\"C-GAN Training Complete\")\n",
    "else:\n",
    "    if architecture == RESNET:\n",
    "        generator = nn.DataParallel(ResGenerator(\n",
    "            LATENT_DIM, NUM_CLASSES, CHANNELS)).to(device)\n",
    "    else:\n",
    "        generator = ConditionalGenerator(\n",
    "            LATENT_DIM, NUM_CLASSES, CHANNELS).to(device)\n",
    "\n",
    "    generator.load_state_dict(torch.load(\n",
    "        gan_model_path, weights_only=True))\n",
    "\n",
    "\n",
    "generate_and_save_images(generator, N_EPOCHS, display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForADRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
